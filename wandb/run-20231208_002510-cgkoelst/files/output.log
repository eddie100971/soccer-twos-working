[INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0
[INFO] Connected new brain: SoccerTwos?team=1
[INFO] Connected new brain: SoccerTwos?team=0
Number of Agents: 4, State Size: 336, Action Size: 3
INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0
INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1
INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0
Rollout 1/49 Time:  9.602636337280273
Trying to log variance and policyloss
Trying to log variance and policyloss
[1mEpisode 2 - Mean Max Reward: 0.00
Mean Reward Agent_0: 0.00, Mean Reward Agent_1: 0.00,
Mean Reward Team 1: 0.00
Mean Reward Team 2: 0.00
Mean Total Reward: 0.00, Mean Episode Length 1500.0
Trying to log variance and policyloss
Trying to log variance and policyloss
[1mEpisode 3 - Mean Max Reward: 0.21
Mean Reward Agent_0: 0.21, Mean Reward Agent_1: 0.21,
Mean Reward Team 1: 0.21
Mean Reward Team 2: -0.33
Mean Total Reward: -0.26, Mean Episode Length 1500.0
Udating Utility Table
Epoch: 1
Rollout 2/49 Time:  12.519836902618408
Rollout 3/49 Time:  11.975802421569824
c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\mappo\ppo_model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  action_mu, action_sigma = self.actor(torch.tensor(state))
c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\mappo\ppo_model.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(action), log_prob
Rollout 4/49 Time:  13.025721073150635
[0.0, 0.0]
[0.0, 0.0]
benchmarking...
[1mEpisode 7 - Mean Max Reward: 0.11
Mean Reward Agent_0: 0.11, Mean Reward Agent_1: 0.11,
Mean Reward Team 1: 0.11
Mean Reward Team 2: -0.29
Mean Total Reward: -0.36, Mean Episode Length 1500.0
[1mEpisode 8 - Mean Max Reward: 0.09
Mean Reward Agent_0: 0.09, Mean Reward Agent_1: 0.09,
Mean Reward Team 1: 0.09
Mean Reward Team 2: -0.25
Mean Total Reward: -0.31, Mean Episode Length 1500.0
Trying to log variance and policyloss
Trying to log variance and policyloss
[1mEpisode 9 - Mean Max Reward: 0.10
Mean Reward Agent_0: 0.10, Mean Reward Agent_1: 0.10,
Mean Reward Team 1: 0.10
Mean Reward Team 2: -0.33
Mean Total Reward: -0.46, Mean Episode Length 1500.0
Trying to log variance and policyloss
Trying to log variance and policyloss
[1mEpisode 10 - Mean Max Reward: 0.18
Mean Reward Agent_0: 0.18, Mean Reward Agent_1: 0.18,
Mean Reward Team 1: 0.18
Mean Reward Team 2: -0.40
Mean Total Reward: -0.43, Mean Episode Length 1500.0
Udating Utility Table
Epoch: 2
Rollout 5/49 Time:  12.166813611984253
Rollout 6/49 Time:  11.800794839859009
Rollout 7/49 Time:  9.912662982940674
Rollout 8/49 Time:  11.968801259994507
Rollout 9/49 Time:  11.932802200317383
[0.0, 0.0, 0.0]
[0.0, 0.0, 0.0]
[0.0, -1.0, 0.0]
benchmarking...
[1mEpisode 16 - Mean Max Reward: 0.18
Mean Reward Agent_0: 0.05, Mean Reward Agent_1: 0.05,
Mean Reward Team 1: 0.05
Mean Reward Team 2: -0.19
Mean Total Reward: -0.27, Mean Episode Length 1500.0
[1mEpisode 17 - Mean Max Reward: 0.17
Mean Reward Agent_0: 0.05, Mean Reward Agent_1: 0.05,
Mean Reward Team 1: 0.05
Mean Reward Team 2: -0.18
Mean Total Reward: -0.26, Mean Episode Length 1500.0
