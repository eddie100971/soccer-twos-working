[INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0
[INFO] Connected new brain: SoccerTwos?team=1
[INFO] Connected new brain: SoccerTwos?team=0
Number of Agents: 4, State Size: 336, Action Size: 3
INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0
INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1
INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0
Rollout 1/35 Time:  8.248568773269653
[1mEpisode 2 - Mean Max Reward: 0.21
Mean Reward Agent_0: 0.21, Mean Reward Agent_1: 0.21,
Mean Reward Team 1: 0.21
Mean Reward Team 2: -0.50
Mean Total Reward: -0.57, Mean Episode Length 1500.0
[1mEpisode 3 - Mean Max Reward: 0.21
Mean Reward Agent_0: -0.19, Mean Reward Agent_1: -0.19,
Mean Reward Team 1: -0.19
Mean Reward Team 2: -0.27
Mean Total Reward: -0.92, Mean Episode Length 1500.0
Udating Utility Table
Epoch: 1
Rollout 2/35 Time:  9.796464204788208
Rollout 3/35 Time:  8.216389417648315
c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\mappo\ppo_model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  action_mu, action_sigma = self.actor(torch.tensor(state))
c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\mappo\ppo_model.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(action), log_prob
Rollout 4/35 Time:  9.510839939117432
[0.0, 1.0]
[0.0, 0.0]
benchmarking...
Traceback (most recent call last):
  File "c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\run_soccer_twos_main.py", line 418, in <module>
    print(trainer.run(6))
  File "c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\run_soccer_twos_main.py", line 388, in run
    self.benchmark()
  File "c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\run_soccer_twos_main.py", line 337, in benchmark
    self.step(train_agents=False)
  File "c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\mappo\mappo_trainer.py", line 173, in step
    scores, utility = self.run_episode(train_agents=train_agents)
  File "c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\mappo\mappo_trainer.py", line 154, in run_episode
    agent.add_memory(state, action, log_prob, reward, False)
AttributeError: 'Opponent' object has no attribute 'add_memory'
[1mEpisode 7 - Mean Max Reward: 0.10
Mean Reward Agent_0: -0.07, Mean Reward Agent_1: -0.07,
Mean Reward Team 1: -0.07
Mean Reward Team 2: -0.26
