[INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0
[INFO] Connected new brain: SoccerTwos?team=1
[INFO] Connected new brain: SoccerTwos?team=0
Number of Agents: 4, State Size: 336, Action Size: 3
INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0
INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1
INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0
Traceback (most recent call last):
  File "c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\run_soccer_twos_main.py", line 414, in <module>
    trainer = create_trainer(env, agents, opponents, save_dir, state_size, action_size, use_PSRO=True, update_frequency=1500)
  File "c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\run_soccer_twos_main.py", line 168, in create_trainer
    trainer = PSRO(
  File "c:\Users\nmone\OneDrive\Desktop\CS\soccer-twos-working\mappo-competitive-reinforcement\run_soccer_twos_main.py", line 299, in __init__
    self.benchmark_opponents = tuple(create_opponent(state_size, action_size, epoch=8500, agent_ix=2), create_opponent(state_size, action_size, epoch=8500, agent_ix=3))
